{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Streaming Data\n",
    "\n",
    "Learning Objectives\n",
    " 1. Learn how to process real-time data for ML models using Cloud Dataflow\n",
    " 2. Learn how to serve online predictions using real-time data\n",
    "\n",
    "## Introduction\n",
    "\n",
    "It can be useful to leverage real time data in a machine learning model when making a prediction. However, doing so requires setting up a streaming data pipeline which can be non-trivial. \n",
    "\n",
    "Typically you will have the following:\n",
    " - A series of IoT devices generating and sending data from the field in real-time (in our case these are the taxis)\n",
    " - A messaging bus to that receives and temporarily stores the IoT data (in our case this is Cloud Pub/Sub)\n",
    " - A streaming processing service that subscribes to the messaging bus, windows the messages and performs data transformations on each window (in our case this is Cloud Dataflow)\n",
    " - A persistent store to keep the processed data (in our case this is BigQuery)\n",
    "\n",
    "These steps happen continuously and in real-time, and are illustrated by the blue arrows in the diagram below. \n",
    "\n",
    "Once this streaming data pipeline is established, we need to modify our model serving to leverage it. This simply means adding a call to the persistent store (BigQuery) to fetch the latest real-time data when a prediction request comes in. This flow is illustrated by the red arrows in the diagram below. \n",
    "\n",
    "<img src='../assets/taxi_streaming_data.png' width='80%'>\n",
    "\n",
    "\n",
    "In this lab we will address how to process real-time data for machine learning models. We will use the same data as our previous 'taxifare' labs, but with the addition of `trips_last_5min` data as an additional feature. This is our proxy for real-time traffic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Collecting apache-beam[gcp]\n",
      "  Using cached apache_beam-2.33.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
      "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (0.19.1)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2.6.0)\n",
      "Requirement already satisfied: orjson<4.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (3.6.3)\n",
      "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.9.2.1)\n",
      "Requirement already satisfied: oauth2client<5,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (4.1.3)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (0.3.1.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2.25.1)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2021.1)\n",
      "Requirement already satisfied: numpy<1.21.0,>=1.14.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.19.5)\n",
      "Requirement already satisfied: pyarrow<5.0.0,>=0.15.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (0.17.1)\n",
      "Requirement already satisfied: future<1.0.0,>=0.18.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (0.18.2)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.4.2)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (3.10.0.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2.8.2)\n",
      "Requirement already satisfied: grpcio<2,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.38.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.12.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (3.18.0)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.7)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (3.12.0)\n",
      "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.7.0)\n",
      "Requirement already satisfied: google-cloud-dlp<2,>=0.12.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.0.0)\n",
      "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (0.5.31)\n",
      "Requirement already satisfied: google-cloud-spanner<2,>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.19.1)\n",
      "Requirement already satisfied: google-cloud-datastore<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.15.3)\n",
      "Requirement already satisfied: cachetools<5,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (4.2.2)\n",
      "Requirement already satisfied: google-cloud-language<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.3.0)\n",
      "Requirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (0.2.2)\n",
      "Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.7.2)\n",
      "Requirement already satisfied: google-cloud-recommendations-ai<=0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (0.2.0)\n",
      "Requirement already satisfied: google-cloud-pubsub<2,>=0.39.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.7.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.35.0)\n",
      "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.16.1)\n",
      "Requirement already satisfied: google-cloud-vision<2,>=0.38.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (1.0.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<3,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]) (2.26.0)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (0.16.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]) (4.7.2)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]) (58.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]) (0.2.7)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (2.0.3)\n",
      "Requirement already satisfied: proto-plus>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (1.19.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.29.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (1.31.2)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (21.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (1.53.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]) (0.12.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3,>=1.6.0->apache-beam[gcp]) (1.2.0)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]) (0.6.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<0.20.0,>=0.8->apache-beam[gcp]) (2.4.7)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]) (1.26.6)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Installing collected packages: apache-beam\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx 0.26.3 requires attrs<21,>=19.3.0, but you have attrs 21.2.0 which is incompatible.\n",
      "tfx 0.26.3 requires click<8,>=7, but you have click 8.0.1 which is incompatible.\n",
      "tfx 0.26.3 requires docker<5,>=4.1, but you have docker 5.0.2 which is incompatible.\n",
      "tfx 0.26.3 requires google-api-python-client<2,>=1.7.8, but you have google-api-python-client 2.22.0 which is incompatible.\n",
      "tfx 0.26.3 requires kubernetes<12,>=10.0.1, but you have kubernetes 18.20.0 which is incompatible.\n",
      "tfx-bsl 0.26.1 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.22.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.26.1 requires apache-beam[gcp]!=2.26.*,<2.29,>=2.25, but you have apache-beam 2.33.0 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires apache-beam[gcp]!=2.26.*,<2.29,>=2.25, but you have apache-beam 2.33.0 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires joblib<0.15,>=0.12, but you have joblib 1.0.1 which is incompatible.\u001b[0m\n",
      "Successfully installed apache-beam-2.33.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.6 MB 8.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.7/site-packages (from pyarrow) (1.19.5)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "Installing collected packages: pyarrow\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tfx 0.26.3 requires attrs<21,>=19.3.0, but you have attrs 21.2.0 which is incompatible.\n",
      "tfx 0.26.3 requires click<8,>=7, but you have click 8.0.1 which is incompatible.\n",
      "tfx 0.26.3 requires docker<5,>=4.1, but you have docker 5.0.2 which is incompatible.\n",
      "tfx 0.26.3 requires google-api-python-client<2,>=1.7.8, but you have google-api-python-client 2.22.0 which is incompatible.\n",
      "tfx 0.26.3 requires kubernetes<12,>=10.0.1, but you have kubernetes 18.20.0 which is incompatible.\n",
      "tfx 0.26.3 requires pyarrow<0.18,>=0.17, but you have pyarrow 5.0.0 which is incompatible.\n",
      "tfx-bsl 0.26.1 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.22.0 which is incompatible.\n",
      "tfx-bsl 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 5.0.0 which is incompatible.\n",
      "tensorflow-transform 0.26.0 requires pyarrow<0.18,>=0.17, but you have pyarrow 5.0.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.26.1 requires apache-beam[gcp]!=2.26.*,<2.29,>=2.25, but you have apache-beam 2.33.0 which is incompatible.\n",
      "tensorflow-model-analysis 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 5.0.0 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires apache-beam[gcp]!=2.26.*,<2.29,>=2.25, but you have apache-beam 2.33.0 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires joblib<0.15,>=0.12, but you have joblib 1.0.1 which is incompatible.\n",
      "tensorflow-data-validation 0.26.1 requires pyarrow<0.18,>=0.17, but you have pyarrow 5.0.0 which is incompatible.\n",
      "apache-beam 2.33.0 requires pyarrow<5.0.0,>=0.15.1, but you have pyarrow 5.0.0 which is incompatible.\u001b[0m\n",
      "Successfully installed pyarrow-5.0.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/opt/conda/lib/python3.7/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restart the kernel before proceeding further (On the Notebook menu - Kernel - Restart Kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from google import api_core\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import bigquery\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Dense, DenseFeatures\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT=qwiklabs-gcp-00-eeb852ce8ccb\n",
      "env: BUCKET=qwiklabs-gcp-00-eeb852ce8ccb\n",
      "env: REGION=us-central1\n"
     ]
    }
   ],
   "source": [
    "# Change below if necessary\n",
    "PROJECT = !gcloud config get-value project  # noqa: E999\n",
    "PROJECT = PROJECT[0]\n",
    "BUCKET = PROJECT\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "%env PROJECT=$PROJECT\n",
    "%env BUCKET=$BUCKET\n",
    "%env REGION=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [ai/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set ai/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-train our model with `trips_last_5min` feature\n",
    "\n",
    "In this lab, we want to show how to process real-time data for training and prediction. So, we need to retrain our previous model with this additional feature. Go through the notebook `4a_streaming_data_training.ipynb`. Open and run the notebook to train and save a model. This notebook is very similar to what we did in the Introduction to Tensorflow module but note the added feature for `trips_last_5min` in the model and the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Real Time Taxi Data\n",
    "\n",
    "Since we don’t actually have real-time taxi data we will synthesize it using a simple python script. The script publishes events to Google Cloud Pub/Sub.\n",
    "\n",
    "Inspect the `iot_devices.py` script in the `taxicab_traffic` folder. It is configured to send about 2,000 trip messages every five minutes with some randomness in the frequency to mimic traffic fluctuations. These numbers come from looking at the historical average of taxi ride frequency in BigQuery. \n",
    "\n",
    "In production this script would be replaced with actual taxis with IoT devices sending trip data to Cloud Pub/Sub. \n",
    "\n",
    "To execute the `iot_devices.py` script, launch a terminal and navigate to the `asl-ml-immersion/notebooks/building_production_ml_systems/solutions` directory. Then run the following two commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "PROJECT_ID=$(gcloud config get-value project)\n",
    "python3 ./taxicab_traffic/iot_devices.py --project=$PROJECT_ID\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see new messages being published every 5 seconds. **Keep this terminal open** so it continues to publish events to the Pub/Sub topic. If you open [Pub/Sub in your Google Cloud Console](https://console.cloud.google.com/cloudpubsub/topic/list), you should be able to see a topic called `taxi_rides`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a BigQuery table to collect the processed data\n",
    "\n",
    "In the next section, we will create a dataflow pipeline to write processed taxifare data to a BigQuery Table, however that table does not yet exist. Execute the following commands to create a BigQuery dataset called `taxifare` and a table within that dataset called `traffic_realtime`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists.\n"
     ]
    }
   ],
   "source": [
    "bq = bigquery.Client()\n",
    "\n",
    "dataset = bigquery.Dataset(bq.dataset(\"taxifare\"))\n",
    "try:\n",
    "    bq.create_dataset(dataset)  # will fail if dataset already exists\n",
    "    print(\"Dataset created.\")\n",
    "except api_core.exceptions.Conflict:\n",
    "    print(\"Dataset already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create a table called `traffic_realtime` and set up the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created.\n"
     ]
    }
   ],
   "source": [
    "dataset = bigquery.Dataset(bq.dataset(\"taxifare\"))\n",
    "\n",
    "table_ref = dataset.table(\"traffic_realtime\")\n",
    "SCHEMA = [\n",
    "    bigquery.SchemaField(\"trips_last_5min\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"time\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n",
    "]\n",
    "table = bigquery.Table(table_ref, schema=SCHEMA)\n",
    "\n",
    "try:\n",
    "    bq.create_table(table)\n",
    "    print(\"Table created.\")\n",
    "except api_core.exceptions.Conflict:\n",
    "    print(\"Table already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch Streaming Dataflow Pipeline\n",
    "\n",
    "Now that we have our taxi data being pushed to Pub/Sub, and our BigQuery table set up, let’s consume the Pub/Sub data using a streaming DataFlow pipeline.\n",
    "\n",
    "The pipeline is defined in `./taxicab_traffic/streaming_count.py`. Open that file and inspect it. \n",
    "\n",
    "There are 5 transformations being applied:\n",
    " - Read from PubSub\n",
    " - Window the messages\n",
    " - Count number of messages in the window\n",
    " - Format the count for BigQuery\n",
    " - Write results to BigQuery\n",
    "\n",
    "**TODO:** Open the file ./taxicab_traffic/streaming_count.py and find the TODO there. Specify a sliding window that is 5 minutes long, and gets recalculated every 15 seconds. Hint: Reference the [beam programming guide](https://beam.apache.org/documentation/programming-guide/#windowing) for guidance. To check your answer reference the solution. \n",
    "\n",
    "For the second transform, we specify a sliding window that is 5 minutes long, and recalculate values every 15 seconds. \n",
    "\n",
    "In a new terminal, launch the dataflow pipeline using the command below. You can change the `BUCKET` variable, if necessary. Here it is assumed to be your `PROJECT_ID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "PROJECT_ID=$(gcloud config get-value project)\n",
    "REGION=$(gcloud config get-value ai/region)\n",
    "BUCKET=$PROJECT_ID # change as necessary \n",
    "python3 ./taxicab_traffic/streaming_count.py \\\n",
    "    --input_topic taxi_rides \\\n",
    "    --runner=DataflowRunner \\\n",
    "    --project=$PROJECT_ID \\\n",
    "    --region=$REGION \\\n",
    "    --temp_location=gs://$BUCKET/dataflow_streaming\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've submitted the command above you can examine the progress of that job in the [Dataflow section of Cloud console](https://console.cloud.google.com/dataflow). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data in the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few moments, you should also see new data written to your BigQuery table as well. \n",
    "\n",
    "Re-run the query periodically to observe new data streaming in! You should see a new row every 15 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "flake8-noqa-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 2/2 [00:00<00:00, 812.30query/s]                         \n",
      "Downloading: 100%|██████████| 10/10 [00:01<00:00,  6.55rows/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trips_last_5min</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>2021-10-11 18:42:25+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1370</td>\n",
       "      <td>2021-10-11 18:42:09+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1301</td>\n",
       "      <td>2021-10-11 18:42:09+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1212</td>\n",
       "      <td>2021-10-11 18:41:40+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1097</td>\n",
       "      <td>2021-10-11 18:41:24+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1015</td>\n",
       "      <td>2021-10-11 18:41:24+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>229</td>\n",
       "      <td>2021-10-11 18:41:20+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>873</td>\n",
       "      <td>2021-10-11 18:41:20+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>152</td>\n",
       "      <td>2021-10-11 18:41:20+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>48</td>\n",
       "      <td>2021-10-11 18:41:20+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   trips_last_5min                      time\n",
       "0             1461 2021-10-11 18:42:25+00:00\n",
       "1             1370 2021-10-11 18:42:09+00:00\n",
       "2             1301 2021-10-11 18:42:09+00:00\n",
       "3             1212 2021-10-11 18:41:40+00:00\n",
       "4             1097 2021-10-11 18:41:24+00:00\n",
       "5             1015 2021-10-11 18:41:24+00:00\n",
       "6              229 2021-10-11 18:41:20+00:00\n",
       "7              873 2021-10-11 18:41:20+00:00\n",
       "8              152 2021-10-11 18:41:20+00:00\n",
       "9               48 2021-10-11 18:41:20+00:00"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  `taxifare.traffic_realtime`\n",
    "ORDER BY\n",
    "  time DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions from the new data\n",
    "\n",
    "In the rest of the lab, we'll referece the model we trained and deployed from the previous labs, so make sure you have run the code in the `4a_streaming_data_training.ipynb` notebook. \n",
    "\n",
    "The `add_traffic_last_5min` function below will query the `traffic_realtime` table to find the most recent traffic information and add that feature to our instance for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Complete the code in the function below. Write a SQL query that will return the most recent entry in `traffic_realtime` and add it to the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2a. Write a function to take most recent entry in `traffic_realtime`\n",
    "# table and add it to instance.\n",
    "def add_traffic_last_5min(instance):\n",
    "    bq = bigquery.Client()\n",
    "    query_string = \"\"\"\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      `taxifare.traffic_realtime`\n",
    "    ORDER BY\n",
    "      time DESC\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    trips = bq.query(query_string).to_dataframe()[\"trips_last_5min\"][0]\n",
    "    instance['traffic_last_5min'] =  int(trips)\n",
    "    return instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `traffic_realtime` table is updated in realtime using Cloud Pub/Sub and Dataflow so, if you run the cell below periodically, you should see the `traffic_last_5min` feature added to the instance and change over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dayofweek': 4,\n",
       " 'hourofday': 13,\n",
       " 'pickup_longitude': -73.99,\n",
       " 'pickup_latitude': 40.758,\n",
       " 'dropoff_latitude': 41.742,\n",
       " 'dropoff_longitude': -73.07,\n",
       " 'traffic_last_5min': 1461}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_traffic_last_5min(\n",
    "    instance={\n",
    "        \"dayofweek\": 4,\n",
    "        \"hourofday\": 13,\n",
    "        \"pickup_longitude\": -73.99,\n",
    "        \"pickup_latitude\": 40.758,\n",
    "        \"dropoff_latitude\": 41.742,\n",
    "        \"dropoff_longitude\": -73.07,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll use the python api to call predictions on an instance, using the realtime traffic information in our prediction. Just as above, you should notice that our resulting predicitons change with time as our realtime traffic information changes as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Complete the code below to call prediction on an instance incorporating realtime traffic info. You should\n",
    "- use the function `add_traffic_last_5min` to add the most recent realtime traffic data to the prediction instance\n",
    "- call prediction on your model for this realtime instance and save the result as a variable called `response`\n",
    "- parse the json of `response` to print the predicted taxifare cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the `ENDPOINT_RESOURCENAME` from the deployment in the previous lab to the beginning of the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " prediction: 15.5426579\n"
     ]
    }
   ],
   "source": [
    "# TODO 2b. Write code to call prediction on instance using realtime traffic\n",
    "# info. Hint: Look at this sample\n",
    "# https://github.com/googleapis/python-aiplatform/blob/master/samples/snippets/predict_custom_trained_model_sample.py\n",
    "\n",
    "# TODO: Copy the `ENDPOINT_RESOURCENAME` from the deployment in the previous\n",
    "# lab.\n",
    "ENDPOINT_RESOURCENAME = \"projects/432069008306/locations/us-central1/endpoints/5048517590096281600\"\n",
    "\n",
    "api_endpoint = f\"{REGION}-aiplatform.googleapis.com\"\n",
    "\n",
    "# The AI Platform services require regional API endpoints.\n",
    "client_options = {\"api_endpoint\": api_endpoint}\n",
    "# Initialize client that will be used to create and send requests.\n",
    "# This client only needs to be created once, and can be reused for multiple\n",
    "# requests.\n",
    "client = aiplatform.gapic.PredictionServiceClient(\n",
    "    client_options=client_options\n",
    ")\n",
    "\n",
    "instance = {\n",
    "    \"dayofweek\": 4,\n",
    "    \"hourofday\": 13,\n",
    "    \"pickup_longitude\": -73.99,\n",
    "    \"pickup_latitude\": 40.758,\n",
    "    \"dropoff_latitude\": 41.742,\n",
    "    \"dropoff_longitude\": -73.07,\n",
    "}\n",
    "\n",
    "# The format of each instance should conform to the deployed model's\n",
    "# prediction input schema.\n",
    "instance_dict = add_traffic_last_5min(instance)\n",
    "\n",
    "instance = json_format.ParseDict(instance, Value())\n",
    "instances = [instance]\n",
    "response = client.predict(endpoint=ENDPOINT_RESOURCENAME, instances=instances)\n",
    "\n",
    "# The predictions are a google.protobuf.Value representation of the model's\n",
    "# predictions.\n",
    "print(\" prediction:\", response.predictions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid ongoing charges, when you are finished with this lab, you can delete your Dataflow job of that job from the [Dataflow section of Cloud console](https://console.cloud.google.com/dataflow).\n",
    "\n",
    "An endpoint with a model deployed to it incurs ongoing charges, as there must be at least one replica defined (the `min-replica-count` parameter is at least 1). In order to stop incurring charges, you can click on the endpoint on the [Endpoints page of the Cloud Console](https://console.cloud.google.com/vertex-ai/endpoints) and un-deploy your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-3.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m80"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
